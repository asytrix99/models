{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7c7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Change: X, y\n",
    "X = np.array([[1,0.5,1.2,-0.3],[1,1,0.8,1.5],[1,2.3,-0.7,0.5],[1,0,1.5,-1]])\n",
    "y = np.array([1,2,3,1]).reshape(-1,1)\n",
    "\n",
    "# Change: initialized weights for each layer\n",
    "# W1: weights from input to first hidden layer\n",
    "# W2: weights from first hidden to output layer\n",
    "# Add W3, W4, etc. for deeper networks\n",
    "W1=np.array([[0,0,0],[0.01,-0.02,0.03],[0.05,0.04,-0.01]])\n",
    "W2=W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d90aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def ReLU_derivative(o):\n",
    "    return (o > 0).astype(float)\n",
    "  \n",
    "def squared_error_loss_derivative(y, y_hat):\n",
    "    return 2*(y_hat-y)\n",
    "\n",
    "def softmax(Z, W, X):\n",
    "    z = X @ W\n",
    "    z_max = np.max(z, axis=1, keepdims=True)  # return maximum value per row (axis=1: row-wise); \n",
    "                                              # keepdims=True: keep the dimension (2D) for later broadcasting\n",
    "    exp_z = np.exp(z - z_max) # prevent overflow\n",
    "    pred_Y = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    # Clip predictions to prevent log(0)\n",
    "    pred_Y = np.clip(pred_Y, eps, 1 - eps)\n",
    "\n",
    "    cost   = np.sum(-(Y * np.log(pred_Y)))/X.shape[0]\n",
    "    gradient = X.T @ (pred_Y-Y) / X.shape[0]\n",
    "\n",
    "    return pred_Y, cost, gradient\n",
    "\n",
    "\n",
    "#   def forward_pass(X, W1, W2, W3):\n",
    "#   return y_hat, A2, O1, A3, O2\n",
    "def forward_pass(X, W1, W2):\n",
    "    O1=ReLU(X @ W1)\n",
    "    A2 = np.column_stack([np.ones((O1.shape[0], 1)), O1])\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    # O2=ReLU(A2 @ W2)\n",
    "    # A3 = np.column_stack([np.ones((O2.shape[0], 1)), O2])\n",
    "    \n",
    "    # Hidden layer 3    \n",
    "    # O3=ReLU(A3 @ W3)\n",
    "    # A4 = np.column_stack([np.ones((O3.shape[0], 1)), O3])\n",
    "\n",
    "    y_hat = A2 @ W2\n",
    "    \n",
    "    # return y_hat, A2, O1, A3, O2, ..., A_n, O_{n-1}\n",
    "    return y_hat, A2, O1\n",
    "\n",
    "def backward_pass_output(y, y_hat, A_o, W_o, lr):\n",
    "    N = y.shape[0]\n",
    "    _,E_o,_ = softmax(y_hat,W_o,A_o)\n",
    "    G_o = (A_o.T @ E_o)/N\n",
    "    W_o_new = W_o - lr * G_o\n",
    "    return E_o, G_o, W_o_new\n",
    "\n",
    "def backward_pass_hidden(E_ladd1, W_ladd1, A_l, O_l, W_l, lr):\n",
    "    N = A_l.shape[0]\n",
    "    E_l = E_ladd1 @ W_ladd1[1:].T * ReLU_derivative(O_l)\n",
    "    G_l = (A_l.T @ E_l)/N\n",
    "    W_l_new = W_l - lr * G_l\n",
    "    return E_l, G_l, W_l_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbc686be",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# USAGE:\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# lr=0.1\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# print(f'Gradient at hidden layer G1 is {G1}')\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(f'Updated W1 is {W1_new}')\u001b[39;00m\n\u001b[32m     22\u001b[39m lr=\u001b[32m0.1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m y_hat, A2, O1=\u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# print(f'Predicted output is {y_hat}')\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# print(f'Input to output layer A2 is {}')\u001b[39;00m\n\u001b[32m     28\u001b[39m E2, G2, W2_new = backward_pass_output(y, y_hat, A2, W2, lr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(X, W1, W2)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_pass\u001b[39m(X, W1, W2):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     O1=ReLU(\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m)\n\u001b[32m     30\u001b[39m     A2 = np.column_stack([np.ones((O1.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m)), O1])\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Hidden layer 2\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# O2=ReLU(A2 @ W2)\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# A3 = np.column_stack([np.ones((O2.shape[0], 1)), O2])\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# O3=ReLU(A3 @ W3)\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# A4 = np.column_stack([np.ones((O3.shape[0], 1)), O3])\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 4)"
     ]
    }
   ],
   "source": [
    "# USAGE:\n",
    "\n",
    "# lr=0.1\n",
    "\n",
    "# y_hat, A2, O1, A3, O2 = forward_pass(X, W1, W2, W3)\n",
    "# y_hat, A2, O1=forward_pass(X, W1, W2)\n",
    "# print(f'Predicted output is {y_hat}')\n",
    "# print(f'Input to output layer A2 is {A2}')\n",
    "\n",
    "# E3, G3, W3_new = backward_pass_output(y, y_hat, A3, W3, lr)\n",
    "# E2, G2, W2_new = backward_pass_output(y, y_hat, A2, W2, lr)\n",
    "# print(f'Error at output layer E2 is {E2}')\n",
    "# print(f'Gradient at output layer G2 is {G2}')\n",
    "# print(f'Updated W2 is {W2_new}')\n",
    "\n",
    "# E2, G2, W2_new = backward_pass_hidden(E3, W3, A2, O2, W2, lr)\n",
    "# E1, G1, W1_new = backward_pass_hidden(E2, W2, X, O1, W1, lr)\n",
    "# print(f'Error at hidden layer E1 is {E1}')\n",
    "# print(f'Gradient at hidden layer G1 is {G1}')\n",
    "# print(f'Updated W1 is {W1_new}')\n",
    "\n",
    "lr=0.1\n",
    "\n",
    "y_hat, A2, O1=forward_pass(X, W1, W2)\n",
    "# print(f'Predicted output is {y_hat}')\n",
    "# print(f'Input to output layer A2 is {}')\n",
    "\n",
    "E2, G2, W2_new = backward_pass_output(y, y_hat, A2, W2, lr)\n",
    "# print(f'Error at output layer E2 is {}')\n",
    "# print(f'Gradient at output layer G2 is {}')\n",
    "# print(f'Updated W2 is {}')\n",
    "\n",
    "E1, G1, W1_new = backward_pass_hidden(E2, W2, X, O1, W1, lr)\n",
    "print(f'Error at hidden layer E1 is {E1}')\n",
    "print(f'Gradient at hidden layer G1 is {G1}')\n",
    "print(f'Updated W1 is {W1_new}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE EXAMPLE: Network with 2 hidden layers (template)\n",
    "# =============================================================================\n",
    "# Forward: X → W1 → O1 → W2 → O2 → W3 → y_hat\n",
    "# Backward: y_hat ← W3 ← O2 ← W2 ← O1 ← W1 ← X\n",
    "#\n",
    "# # Configuration (add at top):\n",
    "# W1 = np.array([...])  # input to hidden1\n",
    "# W2 = np.array([...])  # hidden1 to hidden2\n",
    "# W3 = np.array([...])  # hidden2 to output\n",
    "#\n",
    "# # Forward pass (modify function):\n",
    "# def forward_pass(X, W1, W2, W3):\n",
    "#     O1 = ReLU(X @ W1)\n",
    "#     A2 = np.column_stack([np.ones((O1.shape[0], 1)), O1])\n",
    "#     O2 = ReLU(A2 @ W2)\n",
    "#     A3 = np.column_stack([np.ones((O2.shape[0], 1)), O2])\n",
    "#     y_hat = A3 @ W3\n",
    "#     return y_hat, A2, O1, A3, O2\n",
    "#\n",
    "# # Training:\n",
    "# y_hat, A2, O1, A3, O2 = forward_pass(X, W1, W2, W3)\n",
    "# E3, G3, W3_new = backward_pass_output(y, y_hat, A3, W3, lr)\n",
    "# E2, G2, W2_new = backward_pass_hidden(E3, W3, A2, O2, W2, lr)\n",
    "# E1, G1, W1_new = backward_pass_hidden(E2, W2, X, O1, W1, lr)\n",
    "\n",
    "# =============================================================================\n",
    "# COMPLETE EXAMPLE: Network with 3 hidden layers (template)\n",
    "# =============================================================================\n",
    "# Forward: X → W1 → O1 → W2 → O2 → W3 → O3 → W4 → y_hat\n",
    "# Backward: y_hat ← W4 ← O3 ← W3 ← O2 ← W2 ← O1 ← W1 ← X\n",
    "#\n",
    "# # Configuration (add at top):\n",
    "# W1 = np.array([...])  # input to hidden1\n",
    "# W2 = np.array([...])  # hidden1 to hidden2\n",
    "# W3 = np.array([...])  # hidden2 to hidden3\n",
    "# W4 = np.array([...])  # hidden3 to output\n",
    "#\n",
    "# # Forward pass (modify function):\n",
    "# def forward_pass(X, W1, W2, W3, W4):\n",
    "#     O1 = ReLU(X @ W1)\n",
    "#     A2 = np.column_stack([np.ones((O1.shape[0], 1)), O1])\n",
    "#     O2 = ReLU(A2 @ W2)\n",
    "#     A3 = np.column_stack([np.ones((O2.shape[0], 1)), O2])\n",
    "#     O3 = ReLU(A3 @ W3)\n",
    "#     A4 = np.column_stack([np.ones((O3.shape[0], 1)), O3])\n",
    "#     y_hat = A4 @ W4\n",
    "#     return y_hat, A2, O1, A3, O2, A4, O3\n",
    "#\n",
    "# # Training:\n",
    "# y_hat, A2, O1, A3, O2, A4, O3 = forward_pass(X, W1, W2, W3, W4)\n",
    "# E4, G4, W4_new = backward_pass_output(y, y_hat, A4, W4, lr)\n",
    "# E3, G3, W3_new = backward_pass_hidden(E4, W4, A3, O3, W3, lr)\n",
    "# E2, G2, W2_new = backward_pass_hidden(E3, W3, A2, O2, W2, lr)\n",
    "# E1, G1, W1_new = backward_pass_hidden(E2, W2, X, O1, W1, lr)\n",
    "\n",
    "# =============================================================================\n",
    "# KEY PATTERNS TO REMEMBER\n",
    "# =============================================================================\n",
    "# 1. Forward pass parameters: X, W1, W2, ..., W_n (add weights as needed)\n",
    "# 2. Forward pass returns: y_hat, A2, O1, A3, O2, ..., A_n, O_{n-1}\n",
    "#    - Pattern: y_hat, then alternating A and O (A has bias, O doesn't)\n",
    "# 3. Backward pass always starts with backward_pass_output()\n",
    "# 4. Then call backward_pass_hidden() for each hidden layer in REVERSE order\n",
    "# 5. Each backward_pass_hidden() needs: E_{l+1}, W_{l+1}, A_l, O_l, W_l, lr\n",
    "#    - E and W from the NEXT layer (closer to output)\n",
    "#    - A, O, W from the CURRENT layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
