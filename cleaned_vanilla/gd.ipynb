{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47ba9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, learning_rate, max_iters, i=None):\n",
    "    \"\"\"\n",
    "    f        : function to minimize\n",
    "    grad_f   : gradient of f\n",
    "    x0       : initial point (float or numpy array)\n",
    "    learning_rate : step size\n",
    "    max_iters     : total iterations\n",
    "    i        : if given, return the result at i-th iteration (1-indexed)\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "\n",
    "    history = [x.copy()]\n",
    "    for k in range(0, max_iters + 1):\n",
    "        f_val = f(x)\n",
    "        # Print current iteration\n",
    "        if x.ndim == 0:  # scalar\n",
    "            print(f\"Iter {k:02d}: x={x:.8f}, f(x)={f_val:.8f}\")\n",
    "        else:  # vector\n",
    "            x_str = \"[\" + \", \".join(f\"{xi:.4f}\" for xi in np.ravel(x)) + \"]\"\n",
    "            print(f\"Iter {k:02d}: x={x_str}, f(x)={f_val:.4f}\")\n",
    "\n",
    "        # Gradient update\n",
    "        grad = grad_f(x)\n",
    "        x = x - learning_rate * grad\n",
    "        history.append(x.copy())\n",
    "\n",
    "        if i is not None and k == i:\n",
    "            return x, f(x), k\n",
    "    \n",
    "    return x, f(x), max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8a0531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 00: x=1.00000000, f(x)=1.00000000\n",
      "Iter 01: x=0.20000000, f(x)=0.04000000\n",
      "Iter 02: x=0.04000000, f(x)=0.00160000\n",
      "Iter 03: x=0.00800000, f(x)=0.00006400\n",
      "Iter 04: x=0.00160000, f(x)=0.00000256\n",
      "Iter 05: x=0.00032000, f(x)=0.00000010\n",
      "\n",
      "Final after 5 steps: x=6.399999999999988e-05, f(x)=4.095999999999984e-09\n"
     ]
    }
   ],
   "source": [
    "# Example: f(w) = (x-1)^2 + (y-2)^2\n",
    "f = lambda w: pow(w,2)\n",
    "grad_f = lambda w: 2*w\n",
    "x0 = 1  # start at origin\n",
    "learning_rate = 0.4\n",
    "max_iters = 5\n",
    "# CHANGE THIS\n",
    "\n",
    "x_final, f_final, steps = gradient_descent(f, grad_f, x0, learning_rate, max_iters)\n",
    "print(f\"\\nFinal after {steps} steps: x={x_final}, f(x)={f_final}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
