{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5798f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def projected_gradient_descent(f, grad_f, x0, learning_rate, max_iters, projection, i=None):\n",
    "    \"\"\"\n",
    "    f          : function to minimize\n",
    "    grad_f     : gradient of f\n",
    "    x0         : initial point (float or numpy array)\n",
    "    learning_rate : step size\n",
    "    max_iters     : total iterations\n",
    "    projection : function that projects a point back into the feasible set\n",
    "    i          : if given, return the result at i-th iteration (1-indexed)\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "\n",
    "    history = [x.copy()]\n",
    "    for k in range(0, max_iters + 1):\n",
    "        f_val = f(x)\n",
    "\n",
    "        # Print current state before update\n",
    "        if x.ndim == 0:  # scalar\n",
    "            print(f\"Iter {k:02d}: x={x:.8f}, f(x)={f_val:.8f}\")\n",
    "        else:  # vector\n",
    "            x_str = \"[\" + \", \".join(f\"{xi:.4f}\" for xi in np.ravel(x)) + \"]\"\n",
    "            print(f\"Iter {k:02d}: x={x_str}, f(x)={f_val:.4f}\")\n",
    "\n",
    "        # Gradient update\n",
    "        grad = grad_f(x)\n",
    "        x_new = x - learning_rate * grad\n",
    "        print(f\"   Gradient step: {x_new}\")\n",
    "\n",
    "        # Projection step\n",
    "        x_proj = projection(x_new)\n",
    "        print(f\"   Projected    : {x_proj}\")\n",
    "\n",
    "        # Move to projected point\n",
    "        x = x_proj\n",
    "        history.append(x.copy())\n",
    "\n",
    "        if i is not None and k == i:\n",
    "            return x, f(x), k\n",
    "    \n",
    "    return x, f(x), max_iters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8638b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 00: x=-1.00000000, f(x)=9.00000000\n",
      "   Gradient step: -0.10000000000000009\n",
      "   Projected    : 0.0\n",
      "Iter 01: x=0.00000000, f(x)=4.00000000\n",
      "   Gradient step: 0.6\n",
      "   Projected    : 0.6\n",
      "Iter 02: x=0.60000000, f(x)=1.96000000\n",
      "   Gradient step: 1.02\n",
      "   Projected    : 1.02\n",
      "Iter 03: x=1.02000000, f(x)=0.96040000\n",
      "   Gradient step: 1.314\n",
      "   Projected    : 1.314\n",
      "Iter 04: x=1.31400000, f(x)=0.47059600\n",
      "   Gradient step: 1.5198\n",
      "   Projected    : 1.5198\n",
      "Iter 05: x=1.51980000, f(x)=0.23059204\n",
      "   Gradient step: 1.6638600000000001\n",
      "   Projected    : 1.6638600000000001\n",
      "\n",
      "Final after 5 steps: x=1.6638600000000001, f(x)=0.11299009959999992\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: pow((w-2),2)\n",
    "grad_f = lambda w: 2*(w-2)\n",
    "x0 = -1\n",
    "learning_rate = 0.15\n",
    "max_iters = 5\n",
    "# Projection: enforce non-negativity (x >= 0)\n",
    "projection = lambda w: np.maximum(w, 0)\n",
    "# CHANGE THIS\n",
    "\n",
    "x_final, f_final, steps = projected_gradient_descent(f, grad_f, x0, learning_rate, max_iters, projection)\n",
    "print(f\"\\nFinal after {steps} steps: x={x_final}, f(x)={f_final}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
