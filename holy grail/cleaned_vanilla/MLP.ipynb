{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b388b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_z(X, w): return X @ w\n",
    "def relu(Z): return np.maximum(0,Z)\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z - np.max(Z, axis=-1, keepdims=True))\n",
    "    return e / np.sum(e, axis=-1, keepdims=True)\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "def X_bias(X):\n",
    "    ones=np.ones((X.shape[0], 1))\n",
    "    return np.hstack((ones, X))\n",
    "def err_softmax(A, y): return A - y\n",
    "def err_relu(A, err_next, w_next):\n",
    "    return err_next @ w_next[1:].T * (A > 0)\n",
    "def err_sigmoid(A, err_next, w_next):\n",
    "    return (err_next @ w_next[1:].T) * (A * (1 - A))\n",
    "def err_mse(A, y): return 2 * (A - y)\n",
    "def grad(err, X): return X.T @ err / X.shape[0]\n",
    "def update_weight(w, lr, grad): return w - lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ddab337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00016792 -0.00125257 -0.0010004 ]\n",
      " [ 0.02032425 -0.00924846  0.02722287]\n",
      " [-0.04946549  0.03749486  0.01092531]\n",
      " [ 0.0293399   0.02062628 -0.02024931]]\n",
      "[[-0.00833061 -0.00833205  0.01666266]\n",
      " [ 0.01845714 -0.0086132   0.03015606]\n",
      " [-0.04873108  0.03936466  0.00936642]\n",
      " [ 0.02936574  0.01906595 -0.0184317 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# IF MULTICLASS -> ONEHOT y !\n",
    "\n",
    "X = np.array([[1.2,-0.4,0.8],[-0.6,2,-0.5],[0.3,-1.2,1.7],[2.1,0.5,-0.8]])\n",
    "y = np.array([3,1,2,3]).reshape(-1,1)\n",
    "y_onehot = OneHotEncoder(sparse_output=False).fit_transform(y)\n",
    "W1 = np.array([[0,0,0],[0.02,-0.01,0.03],[-0.05,0.04,0.01],[0.03,0.02,-0.02]])\n",
    "W2 = W1.copy()\n",
    "\n",
    "X_with_bias = X_bias(X)\n",
    "Z1 = get_z(X_with_bias, W1) \n",
    "A1 = relu(Z1)\n",
    "\n",
    "A1_bias = X_bias(A1)\n",
    "Z2 = get_z(A1_bias, W2)\n",
    "A2 = softmax(Z2)\n",
    "\n",
    "E2 = err_softmax(A2, y_onehot)\n",
    "G2 = grad(E2, A1_bias)\n",
    "W2_new = update_weight(W2, 0.1, G2)\n",
    "\n",
    "E1 = err_relu(A1, E2, W2)\n",
    "G1 = grad(E1, X_with_bias)\n",
    "W1_new = update_weight(W1, 0.1, G1)\n",
    "\n",
    "print(W1_new)\n",
    "print(W2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56a3dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1000: cost = 0.10841846755807866\n",
      "iter 2000: cost = 0.004399206130531385\n",
      "iter 3000: cost = 0.0017630665129825228\n",
      "iter 4000: cost = 0.0011347690167500995\n",
      "iter 5000: cost = 0.0008412035250322298\n",
      "iter 6000: cost = 0.0006684415625880973\n",
      "iter 7000: cost = 0.0005557941048293148\n",
      "iter 8000: cost = 0.0004763981615622828\n",
      "iter 9000: cost = 0.00041733777933869334\n",
      "iter 10000: cost = 0.00037157644791852663\n",
      "iter 11000: cost = 0.0003349265923796161\n",
      "iter 12000: cost = 0.00030476453736512555\n",
      "iter 13000: cost = 0.0002796745481219137\n",
      "iter 14000: cost = 0.000258504448029585\n",
      "iter 15000: cost = 0.00024042568748116217\n",
      "iter 16000: cost = 0.00022469509573726934\n",
      "iter 17000: cost = 0.00021095479069977468\n",
      "iter 18000: cost = 0.00019883614568093978\n",
      "iter 19000: cost = 0.0001880587564387737\n",
      "iter 20000: cost = 0.00017840802975499016\n",
      "tr acc: 1.0\n",
      "val acc: 0.8629629629629629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = load_digits()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "poly = PolynomialFeatures(1)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_val_poly = poly.fit_transform(X_val)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "ytr_onehot = encoder.fit_transform(y_train.reshape(-1,1))\n",
    "yval_onehot = encoder.fit_transform(y_val.reshape(-1,1))\n",
    "yts_onehot = encoder.fit_transform(y_test.reshape(-1,1))\n",
    "\n",
    "hidden_layer_size = 32\n",
    "output_layer_size = ytr_onehot.shape[1]\n",
    "lr = 0.01\n",
    "num_iters = 20000\n",
    "\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(X_train_poly.shape[1], hidden_layer_size)\n",
    "W2 = np.random.randn(hidden_layer_size+1, hidden_layer_size)\n",
    "W3 = np.random.randn(hidden_layer_size+1, output_layer_size)\n",
    "\n",
    "def get_z(X, w): return X @ w\n",
    "def relu(Z): return np.maximum(0,Z)\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z - np.max(Z, axis=-1, keepdims=True))\n",
    "    return e / np.sum(e, axis=-1, keepdims=True)\n",
    "def X_bias(X):\n",
    "    ones=np.ones((X.shape[0], 1))\n",
    "    return np.hstack((ones, X))\n",
    "def err_softmax(A, y): return A - y\n",
    "def err_relu(A, err_next, w_next):\n",
    "    return err_next @ w_next[1:].T * (A > 0)\n",
    "def err_mse(A, y): return 2 * (A - y)\n",
    "def cost_softmax(y, A): return -np.mean(np.sum(y * np.log(A + 1e-8), axis=1))\n",
    "def grad(err, X): return X.T @ err / X.shape[0]\n",
    "def update_weight(w, lr, grad): return w - lr * grad\n",
    "\n",
    "for i in range(1,num_iters+1):\n",
    "    \n",
    "    Z1 = get_z(X_train_poly, W1)\n",
    "    A1 = relu(Z1)\n",
    "    A1_bias = X_bias(A1)\n",
    "    Z2 = get_z(A1_bias, W2)\n",
    "    A2 = relu(Z2)\n",
    "    A2_bias = X_bias(A2)\n",
    "    Z3 = get_z(A2_bias, W3)\n",
    "    A3 = softmax(Z3)\n",
    "    \n",
    "    cost = cost_softmax(ytr_onehot, A3)\n",
    "    \n",
    "    E3 = err_softmax(A3, ytr_onehot)\n",
    "    G3 = grad(E3, A2_bias)\n",
    "    W3 = update_weight(W3, lr, G3)\n",
    "    E2 = err_relu(A2, E3, W3)\n",
    "    G2 = grad(E2, A1_bias)\n",
    "    W2 = update_weight(W2, lr, G2)\n",
    "    E1 = err_relu(A1, E2, W2)\n",
    "    G1 = grad(E1, X_train_poly)\n",
    "    W1 = update_weight(W1, lr, G1)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"iter {i}: cost = {cost}\")\n",
    "\n",
    "def predict(X, W1, W2, W3):\n",
    "    Z1 = get_z(X, W1)\n",
    "    A1 = relu(Z1)\n",
    "    A1_bias = X_bias(A1)\n",
    "    Z2 = get_z(A1_bias, W2)\n",
    "    A2 = relu(Z2)\n",
    "    A2_bias = X_bias(A2)\n",
    "    Z3 = get_z(A2_bias, W3)\n",
    "    return softmax(Z3)\n",
    "\n",
    "A3_train = predict(X_train_poly, W1, W2, W3)\n",
    "A3_val = predict(X_val_poly, W1, W2, W3)\n",
    "\n",
    "print(f\"tr acc: {accuracy_score(y_train, np.argmax(A3_train, axis=1))}\")\n",
    "print(f\"val acc: {accuracy_score(y_val, np.argmax(A3_val, axis=1))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
