{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95748ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import log, max, exp, sum, clip\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "\n",
    "# get pred_y, cost, grad (softmax)\n",
    "def multi_logistic_cost_gradient(X, W, Y, eps=1e-15):\n",
    "    z = X @ W\n",
    "    z_max = max(z, axis=1, keepdims=True)   \n",
    "    exp_z = exp(z - z_max)                   \n",
    "    pred_Y = exp_z / sum(exp_z, axis=1, keepdims=True)\n",
    "    pred_Y = clip(pred_Y, eps, 1 - eps)\n",
    "    cost   = sum(-(Y * log(pred_Y)))/ X.shape[0]\n",
    "    gradient = X.T @ (pred_Y-Y)/ X.shape[0]\n",
    "    return pred_Y, cost, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804316ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "X = np.array([[1.2,-0.4,0.8],[-0.6,2,-0.5],[0.3,-1.2,1.7],[2.1,0.5,-0.8]])\n",
    "encoder = PolynomialFeatures(1)\n",
    "P = encoder.fit_transform(X)\n",
    "\n",
    "y = np.array([3,1,2,3]).reshape(-1,1)\n",
    "onehot_encoder=OneHotEncoder(sparse_output=False)\n",
    "Y = onehot_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9f7569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.1244898406408839\n",
      "Initial Gradient = [[ 0.08366715  0.08136882 -0.16503597]\n",
      " [ 0.4076001   0.15942862 -0.56702872]\n",
      " [-0.45593088  0.39346652  0.06246436]\n",
      " [ 0.24832965 -0.33422675  0.08589709]]\n",
      "Initial Weights = [[ 0.    0.    0.  ]\n",
      " [ 0.02 -0.01  0.03]\n",
      " [-0.05  0.04  0.01]\n",
      " [ 0.03  0.02 -0.02]]\n",
      "Iter 1 : cost = 1.0182703437840153\n",
      "Gradient = [[ 0.07058601  0.07498791 -0.14557393]\n",
      " [ 0.37910344  0.15217022 -0.53127367]\n",
      " [-0.42368968  0.36423061  0.05945907]\n",
      " [ 0.2239035  -0.31016662  0.08626313]]\n",
      "Weights = [[-0.00836671 -0.00813688  0.0165036 ]\n",
      " [-0.02076001 -0.02594286  0.08670287]\n",
      " [-0.00440691  0.00065335  0.00375356]\n",
      " [ 0.00516703  0.05342267 -0.02858971]]\n",
      "Iter 2 : cost = 0.9266115384670267\n",
      "Gradient = [[ 0.05955033  0.06871184 -0.12826217]\n",
      " [ 0.35209914  0.14474907 -0.49684821]\n",
      " [-0.39384519  0.33742193  0.05642326]\n",
      " [ 0.20269269 -0.28813985  0.08544715]]\n",
      "Weights = [[-0.01542532 -0.01563567  0.03106099]\n",
      " [-0.05867035 -0.04115988  0.13983024]\n",
      " [ 0.03796206 -0.03576971 -0.00219234]\n",
      " [-0.01722332  0.08443934 -0.03721602]]\n",
      "Iter 1000 : cost = 0.012437823501436327\n",
      "Iter 2000 : cost = 0.006267328548086233\n",
      "Iter 3000 : cost = 0.004187423247620499\n",
      "Iter 4000 : cost = 0.0031438722825730776\n",
      "Iter 5000 : cost = 0.002516701676875377\n",
      "Iter 6000 : cost = 0.002098177830984919\n",
      "Iter 7000 : cost = 0.0017990362948668713\n",
      "Iter 8000 : cost = 0.0015745734570869736\n",
      "Iter 9000 : cost = 0.0013999274028799148\n",
      "Iter 10000 : cost = 0.0012601695365932744\n",
      "Final Cost = 0.0012601695365932744\n",
      "Final Weights = [[ 0.15074023 -0.61606657  0.46532633]\n",
      " [-1.59917588 -2.95580187  4.59497775]\n",
      " [ 2.31892859 -2.48933459  0.170406  ]\n",
      " [-1.05347138  2.42804104 -1.34456965]]\n"
     ]
    }
   ],
   "source": [
    "# learning\n",
    "learning_rate = 0.1\n",
    "W = np.array([[0,0,0],[0.02,-0.01,0.03],[-0.05,0.04,0.01],[0.03,0.02,-0.02]])\n",
    "\n",
    "pred_Y, cost, gradient = multi_logistic_cost_gradient(P, W, Y, eps=1e-15)\n",
    "print('Initial Cost =', cost)\n",
    "print('Initial Gradient =', gradient)\n",
    "print('Initial Weights =', W)\n",
    "\n",
    "num_iters = 10000\n",
    "cost_vec = np.zeros(num_iters+1)\n",
    "cost_vec[0] = cost\n",
    "\n",
    "for i in range(1, num_iters+1):\n",
    "\n",
    "    # update w\n",
    "    W = W - learning_rate*gradient\n",
    "\n",
    "    # compute updated cost and new gradient\n",
    "    pred_Y, cost, gradient = multi_logistic_cost_gradient(P, W, Y, eps=1e-15)\n",
    "    cost_vec[i] = cost\n",
    "\n",
    "    if(i % 1000 == 0):\n",
    "        print('Iter', i, ': cost =', cost)\n",
    "    if(i<3):\n",
    "        print('Iter', i, ': cost =', cost)\n",
    "        print('Gradient =', gradient)\n",
    "        print('Weights =', W)\n",
    "\n",
    "print('Final Cost =', cost)\n",
    "print('Final Weights =', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d372ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m x_test = np.array([-\u001b[32m0.1\u001b[39m]).reshape(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m      3\u001b[39m P_test = encoder.fit_transform(x_test)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m z = \u001b[43mP_test\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\n\u001b[32m      6\u001b[39m z_max = \u001b[38;5;28mmax\u001b[39m(z, axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)   \n\u001b[32m      7\u001b[39m exp_z = exp(z - z_max)  \n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 4 is different from 2)"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "x_test = np.array([-0.1]).reshape(-1,1)\n",
    "P_test = encoder.fit_transform(x_test)\n",
    "\n",
    "z = P_test @ W\n",
    "z_max = max(z, axis=1, keepdims=True)   \n",
    "exp_z = exp(z - z_max)  \n",
    "\n",
    "Y_test = exp_z / sum(exp_z, axis=1, keepdims=True)\n",
    "print(\"z: \" + str(z))\n",
    "print(\"Y_test: \" + str(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
