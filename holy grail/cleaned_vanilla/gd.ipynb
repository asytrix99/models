{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47ba9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, learning_rate, max_iters, i=None):\n",
    "    \"\"\"\n",
    "    f        : function to minimize\n",
    "    grad_f   : gradient of f\n",
    "    x0       : initial point (float or numpy array)\n",
    "    learning_rate : step size\n",
    "    max_iters     : total iterations\n",
    "    i        : if given, return the result at i-th iteration (1-indexed)\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "\n",
    "    history = [x.copy()]\n",
    "    for k in range(0, max_iters + 1):\n",
    "        f_val = f(x)\n",
    "        # Print current iteration\n",
    "        if x.ndim == 0:  # scalar\n",
    "            print(f\"Iter {k:02d}: x={x:.8f}, f(x)={f_val:.8f}\")\n",
    "        else:  # vector\n",
    "            x_str = \"[\" + \", \".join(f\"{xi:.4f}\" for xi in np.ravel(x)) + \"]\"\n",
    "            print(f\"Iter {k:02d}: x={x_str}, f(x)={f_val:.4f}\")\n",
    "\n",
    "        # Gradient update\n",
    "        grad = grad_f(x)\n",
    "        x = x - learning_rate * grad\n",
    "        history.append(x.copy())\n",
    "\n",
    "        if i is not None and k == i:\n",
    "            return x, f(x), k\n",
    "    \n",
    "    return x, f(x), max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60fbc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 00: x=1.00000000, f(x)=1.00000000\n",
      "Iter 01: x=0.95000000, f(x)=0.73509189\n",
      "Iter 02: x=0.91131095, f(x)=0.57279543\n",
      "Iter 03: x=0.87988395, f(x)=0.46403675\n",
      "Iter 04: x=0.85351475, f(x)=0.38660386\n",
      "Iter 05: x=0.83086699, f(x)=0.32899481\n",
      "\n",
      "Final after 5 steps: x=0.8110687106423597, f(x)=0.2846727424487952\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE Only using w\n",
    "\n",
    "f = lambda w: w**6\n",
    "grad_f = lambda w: 5*w**5\n",
    "x0 = 1  # start at origin\n",
    "learning_rate = 0.01\n",
    "max_iters = 5\n",
    "# CHANGE THIS\n",
    "\n",
    "x_final, f_final, steps = gradient_descent(f, grad_f, x0, learning_rate, max_iters)\n",
    "print(f\"\\nFinal after {steps} steps: x={x_final}, f(x)={f_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a0531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 00: x=[0.0000, 0.0000], f(x)=38.0000\n",
      "Iter 01: x=[0.1400, 0.1600], f(x)=33.6152\n",
      "Iter 02: x=[0.2712, 0.3108], f(x)=29.7393\n",
      "Iter 03: x=[0.3941, 0.4529], f(x)=26.3130\n",
      "Iter 04: x=[0.5093, 0.5869], f(x)=23.2842\n",
      "Iter 05: x=[0.6172, 0.7133], f(x)=20.6066\n",
      "\n",
      "Final after 5 steps: x=[0.71824674 0.83240436], f(x)=18.2393701674019\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE LINEAR FUNCTION \n",
    "\n",
    "X = np.array([[0.5,1.2,-0.3],[-1,0.8,1.5],[2.3,-0.7,0.5],[0,1.5,-1]])\n",
    "y = np.array([1,2,3,1]).reshape(-1,1)\n",
    "\n",
    "f = lambda w: np.sum((X @ w - y)**2)\n",
    "grad_f = lambda w: 2 * X.T @ (X @ w - y)\n",
    "x0 = np.array([0,0])  # start at origin\n",
    "learning_rate = 0.01\n",
    "max_iters = 5\n",
    "# CHANGE THIS\n",
    "\n",
    "x_final, f_final, steps = gradient_descent(f, grad_f, x0, learning_rate, max_iters)\n",
    "print(f\"\\nFinal after {steps} steps: x={x_final}, f(x)={f_final}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
