{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47ba9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, learning_rate, max_iters, i=None):\n",
    "    \"\"\"\n",
    "    f        : function to minimize\n",
    "    grad_f   : gradient of f\n",
    "    x0       : initial point (float or numpy array)\n",
    "    learning_rate : step size\n",
    "    max_iters     : total iterations\n",
    "    i        : if given, return the result at i-th iteration (1-indexed)\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "\n",
    "    history = [x.copy()]\n",
    "    for k in range(0, max_iters + 1):\n",
    "        f_val = f(x)\n",
    "        # Print current iteration\n",
    "        if x.ndim == 0:  # scalar\n",
    "            print(f\"Iter {k:02d}: x={x:.8f}, f(x)={f_val:.8f}\")\n",
    "        else:  # vector\n",
    "            x_str = \"[\" + \", \".join(f\"{xi:.4f}\" for xi in np.ravel(x)) + \"]\"\n",
    "            print(f\"Iter {k:02d}: x={x_str}, f(x)={f_val:.4f}\")\n",
    "\n",
    "        # Gradient update\n",
    "        grad = grad_f(x)\n",
    "        x = x - learning_rate * grad\n",
    "        history.append(x.copy())\n",
    "\n",
    "        if i is not None and k == i:\n",
    "            return x, f(x), k\n",
    "    \n",
    "    return x, f(x), max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60fbc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 00: x=3.00000000, f(x)=0.01991486\n",
      "Iter 01: x=3.02794155, f(x)=0.01286106\n",
      "Iter 02: x=3.05047654, f(x)=0.00827920\n",
      "Iter 03: x=3.06859907, f(x)=0.00531861\n",
      "Iter 04: x=3.08314599, f(x)=0.00341212\n",
      "Iter 05: x=3.09480872, f(x)=0.00218714\n",
      "\n",
      "Final after 5 steps: x=3.104151859050176, f(x)=0.001401158191523737\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE Only using w\n",
    "\n",
    "f = lambda w: np.sin(w)**2\n",
    "grad_f = lambda w: np.sin(2*w)\n",
    "x0 = 3  # start at origin\n",
    "learning_rate = 0.1\n",
    "max_iters = 5\n",
    "# CHANGE THIS\n",
    "\n",
    "x_final, f_final, steps = gradient_descent(f, grad_f, x0, learning_rate, max_iters)\n",
    "print(f\"\\nFinal after {steps} steps: x={x_final}, f(x)={f_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a0531",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m max_iters = \u001b[32m5\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# CHANGE THIS\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m x_final, f_final, steps = \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m steps: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_final\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, f(x)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_final\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mgradient_descent\u001b[39m\u001b[34m(f, grad_f, x0, learning_rate, max_iters, i)\u001b[39m\n\u001b[32m     14\u001b[39m history = [x.copy()]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, max_iters + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     f_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Print current iteration\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.ndim == \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# scalar\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(w)\u001b[39m\n\u001b[32m      3\u001b[39m X = np.array([[\u001b[32m0.5\u001b[39m,\u001b[32m1.2\u001b[39m,-\u001b[32m0.3\u001b[39m],[-\u001b[32m1\u001b[39m,\u001b[32m0.8\u001b[39m,\u001b[32m1.5\u001b[39m],[\u001b[32m2.3\u001b[39m,-\u001b[32m0.7\u001b[39m,\u001b[32m0.5\u001b[39m],[\u001b[32m0\u001b[39m,\u001b[32m1.5\u001b[39m,-\u001b[32m1\u001b[39m]])\n\u001b[32m      4\u001b[39m y = np.array([\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m1\u001b[39m]).reshape(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m f = \u001b[38;5;28;01mlambda\u001b[39;00m w: np.sum((\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m - y)**\u001b[32m2\u001b[39m)\n\u001b[32m      7\u001b[39m grad_f = \u001b[38;5;28;01mlambda\u001b[39;00m w: \u001b[32m2\u001b[39m * X.T @ (X @ w - y)\n\u001b[32m      8\u001b[39m x0 = np.array([\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m])  \u001b[38;5;66;03m# start at origin\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)"
     ]
    }
   ],
   "source": [
    "# EXAMPLE LINEAR FUNCTION \n",
    "\n",
    "X = np.array([[0.5,1.2,-0.3],[-1,0.8,1.5],[2.3,-0.7,0.5],[0,1.5,-1]])\n",
    "y = np.array([1,2,3,1]).reshape(-1,1)\n",
    "\n",
    "f = lambda x,y: x**2 + x*y**2\n",
    "grad_f = lambda w: 2 * X.T @ (X @ w - y)\n",
    "x = 3  # start at origin\n",
    "y = 2\n",
    "learning_rate = 0.2\n",
    "max_iters = 5\n",
    "# CHANGE THIS\n",
    "\n",
    "x_final, f_final, steps = gradient_descent(f, grad_f, x0, learning_rate, max_iters)\n",
    "print(f\"\\nFinal after {steps} steps: x={x_final}, f(x)={f_final}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
